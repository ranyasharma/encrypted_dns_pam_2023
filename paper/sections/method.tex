\section{Method}\label{sec:method} In this section, we describe the metrics
used, how these metrics are measured, and our experiment setup.

\subsection{Metrics} 

\paragraph{Availability.} We are interested in determining which DoH
resolvers are still active and responding to queries.  We define a resolver as
unresponsive from a given vantage point if we fail to receive \emph{any}
response to the queries issued from a particular server.

\paragraph{Latency.} We
performed network latency measurements for each recursive resolver.  Each time
we issued a set of DoH queries to a resolver, we also issued a ICMP ping
message and noted the round-trip time.  This enabled us to explore
whether there was a consistent relationship between high query response times
and network latency.

\paragraph{DNS query response time.} We define DNS query response time as the
end-to-end time it takes for a client to initiate a query and receive a
response.  To measure query response times with various DoH resolvers, we
performed dig queries to the resolvers. We used the public, open-source Netrics
platform~\cite{netrics} to conduct periodic measurements. Netrics
provides a variety of open-source network measurement tests, and we added our 
test to this suite. Our tool enables researchers to issue
traditional DNS, DoT, and DoH queries.

Our tool supports continuous DoH response time measurements. Clients to provide
a list of DoH resolvers they wish to perform measurements with. After a set of 
measurements complete with a list of DoH resolvers and domain names, the tool
writes the results to a JSON file.

\subsection{Experiment Setup}
To provide a comparative assessment of DNS
performance across DoH resolvers, we perform measurements across 91 DoH
resolvers, grouped by their geographical locations—18 in North America, 13 in
Asia, and 33 in Europe~\cite{dnscrypt}. See Appendix for the complete list of resolvers. 6 resolvers were unable to return a location. 
We employed MaxMind's GeoLite2 databases to geolocate each DoH resolver~\cite{maxmind}.

We collect measurements via two sources—-home network devices and Amazon EC2 instances. 
Both sources use the same measurement platform. This allows us to explore measurements from a wide variety
of access networks. We performed continuous measurements on four devices deployed in a single apartment complex located in a 
Chicago metropolitan area between June 22--September 30, 2023. We designed our tests to run every few hours. They were measured over IPv4. We also performed our measurements on Amazon EC2 instances between September 19--October 16, 2023. 
After this 1 month experiment span, we continued to take measurements for 1-3 days per month until May 2024. 
The motivation behind this was to ensure that resolver performance did not change drastically since October 2023. 
Those measurement spans were February 8--February 10, 2024, 
March 12--March 13, 2024, April 12--April 14, 2024. Measurements were performed three times a day. 
We also took the four highest performing resolvers (\texttt{Google}, \texttt{Cloudflare}, \texttt{Quad9},
\texttt{Hurricane Electric}) located in North America and measured their
performance in Europe and Asia to better understand how they compare in
farther vantage points. 

\paragraph{Vantage Points.} 
We performed our measurements across four units in the same apartment complex in the Chicagoland area. 
We installed our measurement platform on Raspberry Pi devices, which we used to 
collect data. 

We also performed our measurements from three global
vantage points through Amazon EC2~\cite{amazon_ec2}.  We deployed one server
in each of the Ohio, Frankfurt, and Seoul EC2 regions.  We chose to perform
measurements from multiple global vantage points to understand how DoH
performance varies not only by which resolver is used, but also which
geographic region the client is located in.  Each server utilized 8 GB of RAM
and 2 virtual CPU cores (the \texttt{t2.xlarge} instance type), and they each
used Linux/UNIX~\cite{amazon_ec2_instance_types}.

\paragraph{Resolvers and Domain Names.} In the Appendix, we list each of 
the DoH resolvers we measured.  These resolvers were
scraped from a list of public DoH resolvers provided by the \texttt{DNSCrypt}
protocol developers~\cite{dnscrypt-public-resolvers}.
We chose this list of resolvers because it was publicly available.
Previous work has largely studied major DNS providers in use by web browsers; in contrast, we
measure the performance of a larger set of encrypted DNS resolvers~\cite{hounsel2020comparing,hounsel2021can,hoang2020k,lu2019end-to-end}.

We issued queries for three domains to each resolver: \texttt{google.com}, 
\texttt{amazon.com}, and \texttt{wikipedia.com}.  We chose these domains based on their popularity, but
other domain names would have likely sufficed.  We do not expect our choice of
domain names to unfairly skew our performance comparisons between resolvers. 

\paragraph{Caching}
Note that our selection of domains is quite realistic, because it is reasonable to expect that most people query sites that are already in cache. 
Therefore, in our method, we capture the typical user experience.
We are primarily interested in the behavior of a resolver and the presence of cached entries enables a more controlled experiment.

\paragraph{Measurement Procedure.} We performed the following steps to measure
the performance of each of the encrypted DNS resolvers, from each of our three vantage points:
\begin{enumerate} 
%    \item Pass a list of DoH resolvers and domain names to our
%            modified DoH query response time measurement tool.  
        \item For each resolver that we aim to measure, perform a dig query, measuring the query response time for three
            domain names.
    \item For each resolver, issue a ICMP ping
            probe and collect the round-trip latency. 
\end{enumerate}

\paragraph{Limitations.} Our work has several limitations.
First, we do not measure how encrypted DNS affects application
performance, such as web page load time. Ultimately, an assessment of the
effects of encrypted DNS performance on application performance, including web
page load time, across the full set of encrypted DNS resolvers, could provide
a more comprehensive understanding of the effects of encrypted DNS on
application performance. Furthermore, although we do not expect it to affect conclusions, it may
be informative to perform measurements from a larger set domain names; our
measurements perform DNS lookups to just three domain names.
